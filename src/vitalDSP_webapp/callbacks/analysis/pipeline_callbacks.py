"""
Pipeline visualization and execution callbacks.

This module handles callbacks for the 8-stage processing pipeline page,
including execution, progress tracking, and results visualization.
"""

from dash.dependencies import Input, Output, State
from dash import html, no_update, callback_context
import dash_bootstrap_components as dbc
import plotly.graph_objs as go
import pandas as pd
import numpy as np
import logging
import time
import json

# Import the real pipeline integration service
# TODO: Re-enable after optimizing pipeline initialization
# from vitalDSP_webapp.services.pipeline_integration import get_pipeline_service

logger = logging.getLogger(__name__)


def _get_stage_number(current_stage):
    """
    Extract stage number from current_stage (handles both int and string pipeline_run_id).

    Args:
        current_stage: Either an int (old simulation) or string (pipeline_run_id)

    Returns:
        int: Stage number, or 0 if it's a pipeline_run_id
    """
    if isinstance(current_stage, int):
        return current_stage
    elif isinstance(current_stage, str) and len(current_stage) == 8:
        # It's a pipeline_run_id, get stage from pipeline_data
        if hasattr(register_pipeline_callbacks, 'pipeline_data'):
            pipeline_data = register_pipeline_callbacks.pipeline_data.get(current_stage)
            if pipeline_data:
                return pipeline_data.get('current_stage', 0)
    return 0


def _execute_pipeline_stage(pipeline_data: dict, stage: int, signal_data: np.ndarray, fs: float, signal_type: str):
    """
    Execute a single pipeline stage with real vitalDSP processing.

    Args:
        pipeline_data: Pipeline run data dictionary
        stage: Stage number (1-8)
        signal_data: Input signal data
        fs: Sampling frequency
        signal_type: Signal type (ecg, ppg, etc.)

    Returns:
        tuple: (success: bool, result: dict or error_message: str)
    """
    try:
        if stage == 1:
            # Stage 1: Data Ingestion
            result = {
                'samples': len(signal_data),
                'duration': len(signal_data) / fs,
                'fs': fs,
                'signal_type': signal_type,
                'mean': float(np.mean(signal_data)),
                'std': float(np.std(signal_data)),
                'min': float(np.min(signal_data)),
                'max': float(np.max(signal_data)),
            }
            return True, result

        elif stage == 2:
            # Stage 2: Quality Screening - Use SignalQualityIndex individual methods
            from vitalDSP.signal_quality_assessment.signal_quality_index import SignalQualityIndex

            # Initialize SignalQualityIndex
            sqi = SignalQualityIndex(signal_data)

            # Get parameters from pipeline_data
            sqi_window_seconds = pipeline_data.get('sqi_window', 5)
            quality_threshold = pipeline_data.get('quality_threshold', 0.7)

            # Define window parameters for segment-based SQIs
            window_size = int(sqi_window_seconds * fs)  # Use user-configured window size
            step_size = window_size // 2  # 50% overlap

            # Compute multiple SQI metrics
            quality_results = {}

            try:
                # Baseline wander SQI
                bl_sqi, bl_normal, bl_abnormal = sqi.baseline_wander_sqi(
                    window_size, step_size, threshold=quality_threshold
                )
                quality_results['baseline_wander_sqi'] = float(np.mean(bl_sqi))
            except Exception as e:
                logger.warning(f"Baseline wander SQI failed: {e}")
                quality_results['baseline_wander_sqi'] = 0.5

            try:
                # Amplitude variability SQI
                amp_sqi, amp_normal, amp_abnormal = sqi.amplitude_variability_sqi(
                    window_size, step_size, threshold=quality_threshold
                )
                quality_results['amplitude_variability_sqi'] = float(np.mean(amp_sqi))
            except Exception as e:
                logger.warning(f"Amplitude variability SQI failed: {e}")
                quality_results['amplitude_variability_sqi'] = 0.5

            try:
                # SNR SQI
                snr_sqi, snr_normal, snr_abnormal = sqi.snr_sqi(
                    window_size, step_size, threshold=quality_threshold
                )
                quality_results['snr_sqi'] = float(np.mean(snr_sqi))
            except Exception as e:
                logger.warning(f"SNR SQI failed: {e}")
                quality_results['snr_sqi'] = 0.5

            # Compute overall quality score (average of available SQIs)
            sqi_values = [v for v in quality_results.values() if isinstance(v, (int, float)) and not np.isnan(v)]
            overall_quality = np.mean(sqi_values) if sqi_values else 0.5

            result = {
                'quality_scores': quality_results,
                'overall_quality': float(overall_quality),
                'passed': overall_quality >= quality_threshold,
                'sqi_window': sqi_window_seconds,
                'threshold_used': quality_threshold,
            }
            return True, result

        elif stage == 3:
            # Stage 3: Parallel Processing (Filtering)
            from vitalDSP.filtering.signal_filtering import SignalFiltering

            # Get parameters from pipeline_data
            lowcut = pipeline_data.get('filter_lowcut', 0.5)
            highcut = pipeline_data.get('filter_highcut', 40)
            filter_order = pipeline_data.get('filter_order', 4)
            baseline_cutoff = pipeline_data.get('baseline_cutoff', 0.5)

            paths = pipeline_data.get('paths', ['filtered', 'preprocessed'])
            results = {}

            if 'raw' in paths:
                results['raw'] = signal_data.copy()

            if 'filtered' in paths or 'preprocessed' in paths:
                sf = SignalFiltering(signal_data)
                # Use user-configured filter parameters
                filtered = sf.bandpass(lowcut=lowcut, highcut=highcut, fs=fs, order=filter_order)

                if 'filtered' in paths:
                    results['filtered'] = filtered

                if 'preprocessed' in paths:
                    # Additional artifact removal for preprocessed path
                    from vitalDSP.filtering.artifact_removal import ArtifactRemoval
                    ar = ArtifactRemoval(filtered)  # Only takes signal parameter
                    preprocessed = ar.baseline_correction(cutoff=baseline_cutoff, fs=fs)  # Method takes fs
                    results['preprocessed'] = preprocessed

            result = {
                'paths': list(results.keys()),
                'filtered_samples': {k: len(v) for k, v in results.items()},
                'filter_params': {
                    'lowcut': lowcut,
                    'highcut': highcut,
                    'order': filter_order,
                    'baseline_cutoff': baseline_cutoff,
                },
            }
            # Store processed signals for next stages
            pipeline_data['processed_signals'] = results
            return True, result

        elif stage == 4:
            # Stage 4: Quality Validation (compare paths)
            processed_signals = pipeline_data.get('processed_signals', {})
            quality_scores = {}

            for path_name, path_signal in processed_signals.items():
                snr = float(np.std(path_signal) / (np.std(path_signal - signal_data[:len(path_signal)]) + 1e-10))
                quality_scores[path_name] = max(0, min(1, snr / 10))  # Normalize to 0-1

            best_path = max(quality_scores, key=quality_scores.get) if quality_scores else 'raw'

            result = {
                'quality_scores': quality_scores,
                'best_path': best_path,
                'confidence': quality_scores.get(best_path, 0.5),
            }
            pipeline_data['best_path'] = best_path
            return True, result

        elif stage == 5:
            # Stage 5: Segmentation
            best_path = pipeline_data.get('best_path', 'raw')
            processed_signals = pipeline_data.get('processed_signals', {})
            best_signal = processed_signals.get(best_path, signal_data)

            # Get parameters from pipeline_data
            window_size = pipeline_data.get('window_size', 30)  # seconds
            overlap_ratio = pipeline_data.get('overlap_ratio', 0.5)

            window_samples = int(window_size * fs)
            step_samples = int(window_samples * (1 - overlap_ratio))

            segments = []
            for start in range(0, len(best_signal) - window_samples + 1, step_samples):
                end = start + window_samples
                segments.append(best_signal[start:end])

            result = {
                'num_segments': len(segments),
                'window_size': window_size,
                'overlap_ratio': overlap_ratio,
                'segment_length': window_samples,
            }
            pipeline_data['segments'] = segments
            return True, result

        elif stage == 6:
            # Stage 6: Feature Extraction
            segments = pipeline_data.get('segments', [])
            features = []

            for seg in segments:
                seg_features = {
                    'mean': float(np.mean(seg)),
                    'std': float(np.std(seg)),
                    'rms': float(np.sqrt(np.mean(seg**2))),
                    'max': float(np.max(seg)),
                    'min': float(np.min(seg)),
                    'peak_to_peak': float(np.ptp(seg)),
                }
                features.append(seg_features)

            result = {
                'num_features': len(features),
                'feature_types': list(features[0].keys()) if features else [],
                'features_per_segment': len(features[0]) if features else 0,
            }
            pipeline_data['features'] = features
            return True, result

        elif stage == 7:
            # Stage 7: Intelligent Output
            best_path = pipeline_data.get('best_path', 'raw')
            quality_scores = pipeline_data.get('results', {}).get('stage_4', {}).get('quality_scores', {})

            recommendations = []
            if quality_scores.get(best_path, 0) > 0.7:
                recommendations.append("Signal quality is good - suitable for detailed analysis")
            else:
                recommendations.append("Consider additional preprocessing for better results")

            result = {
                'best_path': best_path,
                'recommendations': recommendations,
                'confidence': quality_scores.get(best_path, 0.5),
            }
            return True, result

        elif stage == 8:
            # Stage 8: Output Package
            result = {
                'status': 'completed',
                'total_stages': 8,
                'processing_time': 'calculated_in_real_time',
                'results_summary': {
                    k: v for k, v in pipeline_data.get('results', {}).items()
                }
            }
            return True, result

        else:
            return False, f"Unknown stage: {stage}"

    except Exception as e:
        logger.error(f"Error in stage {stage}: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False, str(e)


def register_pipeline_callbacks(app):
    """
    Register all pipeline-related callbacks.

    Parameters
    ----------
    app : Dash
        The Dash application instance
    """

    @app.callback(
        [
            Output("pipeline-progress-container", "style"),
            Output("pipeline-processing-progress", "value"),
            Output("pipeline-processing-progress-status", "children"),
            Output("pipeline-run-btn", "disabled"),
            Output("pipeline-stop-btn", "disabled"),
            Output("pipeline-export-btn", "disabled"),
            Output("pipeline-report-btn", "disabled"),
            Output("pipeline-progress-interval", "disabled"),
            Output("pipeline-current-stage", "data", allow_duplicate=True),
        ],
        [
            Input("pipeline-run-btn", "n_clicks"),
            Input("pipeline-stop-btn", "n_clicks"),
            Input("pipeline-reset-btn", "n_clicks"),
            Input("pipeline-progress-interval", "n_intervals"),
        ],
        [
            State("pipeline-signal-type", "value"),
            State("pipeline-paths", "value"),
            State("pipeline-enable-quality", "value"),
            State("pipeline-current-stage", "data"),
            State("store-uploaded-data", "data"),
            # Stage 2 parameters
            State("pipeline-sqi-window", "value"),
            State("pipeline-quality-threshold", "value"),
            # Stage 3 parameters
            State("pipeline-filter-lowcut", "value"),
            State("pipeline-filter-highcut", "value"),
            State("pipeline-filter-order", "value"),
            State("pipeline-baseline-cutoff", "value"),
            # Stage 5 parameters
            State("pipeline-window-size", "value"),
            State("pipeline-overlap-ratio", "value"),
            # Stage 6 parameters
            State("pipeline-feature-types", "value"),
        ],
        prevent_initial_call=True,
    )
    def handle_pipeline_execution(
        run_clicks,
        stop_clicks,
        reset_clicks,
        n_intervals,
        signal_type,
        paths,
        enable_quality,
        current_stage,
        uploaded_data,
        # Stage 2 parameters
        sqi_window,
        quality_threshold,
        # Stage 3 parameters
        filter_lowcut,
        filter_highcut,
        filter_order,
        baseline_cutoff,
        # Stage 5 parameters
        window_size,
        overlap_ratio,
        # Stage 6 parameters
        feature_types,
    ):
        """
        Handle pipeline execution, progress tracking, and control buttons.
        """
        ctx = callback_context

        if not ctx.triggered:
            return (
                {"display": "none"},
                0,
                "",
                False,
                True,
                True,
                True,
                True,
                0,
            )

        trigger_id = ctx.triggered[0]["prop_id"].split(".")[0]

        # Reset button
        if trigger_id == "pipeline-reset-btn":
            return (
                {"display": "none"},
                0,
                "",
                False,
                True,
                True,
                True,
                True,
                0,
            )

        # Run button
        if trigger_id == "pipeline-run-btn":
            # Try to get data from data service (for large files) or store (for small files)
            data_loaded = False
            df = None

            # First, try data service (more reliable for large files)
            try:
                from vitalDSP_webapp.services.data.data_service import get_data_service
                data_service = get_data_service()

                if data_service:
                    # Get all stored data and use the most recent one
                    all_data = data_service.get_all_data()
                    logger.info(f"Data service has {len(all_data)} stored datasets")

                    if all_data:
                        # Get the most recent data ID (highest number)
                        data_ids = sorted(all_data.keys(), reverse=True)
                        latest_id = data_ids[0]
                        logger.info(f"Using latest data ID: {latest_id}")

                        df = data_service.get_data(latest_id)
                        if df is not None:
                            logger.info(f"âœ“ Loaded data from data service: ID={latest_id}, shape={df.shape}, columns={list(df.columns)}")
                            data_loaded = True
                        else:
                            logger.warning(f"Data ID {latest_id} exists but data is None")
                    else:
                        logger.warning("Data service available but no stored data")
                else:
                    logger.warning("Data service is None")
            except Exception as e:
                logger.warning(f"Could not load from data service: {e}")
                import traceback
                logger.warning(f"Traceback: {traceback.format_exc()}")

            # Fallback: try uploaded_data store (for smaller files)
            if not data_loaded and uploaded_data:
                try:
                    import pandas as pd
                    if isinstance(uploaded_data, list) and len(uploaded_data) > 0:
                        df = pd.DataFrame(uploaded_data)
                        logger.info(f"âœ“ Loaded data from store: shape={df.shape}, columns={list(df.columns)}")
                        data_loaded = True
                except Exception as e:
                    logger.warning(f"Could not load from store: {e}")

            # If no data loaded, show error
            if not data_loaded or df is None:
                logger.warning("No uploaded data found in either data service or store")
                return (
                    {"display": "block"},
                    0,
                    "âš ï¸ Error: No data uploaded. Please upload and process data first.",
                    False,
                    True,
                    True,
                    True,
                    True,
                    0,
                )

            # Convert uploaded data to DataFrame for processing
            try:
                logger.info(f"Processing data: shape={df.shape}, columns={list(df.columns)}")

                # Determine signal column (look for common names)
                signal_col = None
                for col_name in ['signal', 'Signal', 'value', 'Value', 'amplitude', 'Amplitude']:
                    if col_name in df.columns:
                        signal_col = col_name
                        break

                if signal_col is None and len(df.columns) > 0:
                    # Use first numeric column
                    for col in df.columns:
                        if pd.api.types.is_numeric_dtype(df[col]):
                            signal_col = col
                            break

                if signal_col is None:
                    raise ValueError("No signal column found in uploaded data")

                signal_data = df[signal_col].values
                logger.info(f"âœ“ Signal data extracted: column='{signal_col}', length={len(signal_data)}, range=[{signal_data.min():.3f}, {signal_data.max():.3f}]")

                # Determine sampling frequency from data config if available
                # Try to get from data service first
                try:
                    from vitalDSP_webapp.services.data.data_service import get_data_service
                    data_service = get_data_service()
                    all_data = data_service.get_all_data()
                    if all_data:
                        latest_id = sorted(all_data.keys(), reverse=True)[0]
                        data_info = data_service.get_data_info(latest_id)
                        fs = data_info.get('sampling_freq', 128) if data_info else 128
                        logger.info(f"âœ“ Sampling frequency from data service: {fs} Hz")
                    else:
                        fs = 128  # Default
                        logger.warning("No data info found, using default fs=128 Hz")
                except Exception as e:
                    logger.warning(f"Could not get sampling freq from data service: {e}")
                    fs = 128  # Default

                # REAL PIPELINE MODE: Process the actual data
                logger.info(f"Starting REAL pipeline processing for signal_type: {signal_type}")
                logger.info(f"Selected paths: {paths}, enable_quality: {enable_quality}")
                logger.info(f"Data: {len(signal_data)} samples at {fs} Hz")
                logger.info(f"Duration: {len(signal_data)/fs:.2f} seconds")

                # Store signal data and config in session for pipeline to access
                # We'll use a simple approach: store in a global dict keyed by a new pipeline_run_id
                import hashlib
                import time
                pipeline_run_id = hashlib.md5(f"{time.time()}".encode()).hexdigest()[:8]

                # Store in module-level dict (we'll create this)
                if not hasattr(register_pipeline_callbacks, 'pipeline_data'):
                    register_pipeline_callbacks.pipeline_data = {}

                register_pipeline_callbacks.pipeline_data[pipeline_run_id] = {
                    'signal_data': signal_data,
                    'fs': fs,
                    'signal_type': signal_type,
                    'paths': paths or ['filtered', 'preprocessed'],
                    'enable_quality': enable_quality,
                    'current_stage': 0,
                    'total_stages': 8,
                    'results': {},
                    # Stage parameters
                    'sqi_window': sqi_window or 5,
                    'quality_threshold': quality_threshold or 0.7,
                    'filter_lowcut': filter_lowcut or 0.5,
                    'filter_highcut': filter_highcut or 40,
                    'filter_order': filter_order or 4,
                    'baseline_cutoff': baseline_cutoff or 0.5,
                    'window_size': window_size or 30,
                    'overlap_ratio': overlap_ratio or 0.5,
                    'feature_types': feature_types or ['time', 'frequency'],
                }

                logger.info(f"Created pipeline run: {pipeline_run_id}")

                return (
                    {"display": "block"},
                    0,  # Start at 0% - data ingestion will begin
                    "Starting real pipeline processing...",
                    True,  # Disable run button
                    False,  # Enable stop button
                    True,  # Disable export button
                    True,  # Disable report button
                    False,  # Enable interval (False = not disabled)
                    pipeline_run_id,  # Store pipeline run ID
                )

            except Exception as e:
                logger.error(f"Error loading uploaded data: {e}")
                import traceback
                logger.error(f"Traceback: {traceback.format_exc()}")
                return (
                    {"display": "block"},
                    0,
                    f"âš ï¸ Error: {str(e)}",
                    False,
                    True,
                    True,
                    True,
                    True,
                    0,
                )

            # TODO: Re-enable data validation when using real pipeline
            # if not uploaded_data:
            #     return (
            #         {"display": "block"},
            #         0,
            #         "Error: No data uploaded",
            #         False,
            #         True,
            #         True,
            #         True,
            #         True,
            #         0,
            #     )
            #
            # # Extract signal data from uploaded data
            # try:
            #     signal_data = np.array(uploaded_data.get('data', []))
            #     fs = uploaded_data.get('fs', 1000)
            #
            #     if len(signal_data) == 0:
            #         return (
            #             {"display": "block"},
            #             0,
            #             "Error: No signal data found",
            #             False,
            #             True,
            #             True,
            #             True,
            #             True,
            #             0,
            #         )
            #
            #     # Real pipeline execution code here
            #     ...
            # except Exception as e:
            #     logger.error(f"Failed to start pipeline execution: {e}")
            #     return (
            #         {"display": "block"},
            #         0,
            #         f"Error starting pipeline: {str(e)}",
            #         False,
            #         True,
            #         True,
            #         True,
            #         True,
            #         0,
            #     )

        # Stop button
        if trigger_id == "pipeline-stop-btn":
            # TODO: Re-enable after optimizing pipeline initialization
            # # Check if we have a real session running
            # if isinstance(current_stage, str) and current_stage.startswith('session_'):
            #     session_id = current_stage.replace('session_', '')
            #     pipeline_service = get_pipeline_service()
            #     stopped = pipeline_service.stop_pipeline_execution(session_id)
            #
            #     if stopped:
            #         return (
            #             {"display": "block"},
            #             current_stage / 8 * 100,
            #             f"Pipeline stopped at current stage",
            #             False,
            #             True,
            #             True,
            #             True,
            #             True,  # Disable interval
            #             current_stage,
            #         )

            # Simulation mode stop
            return (
                {"display": "block"},
                current_stage / 8 * 100 if isinstance(current_stage, int) else 0,
                f"Stopped at stage {current_stage + 1}/8" if isinstance(current_stage, int) else "Stopped",
                False,
                True,
                current_stage == 8 if isinstance(current_stage, int) else True,
                current_stage == 8 if isinstance(current_stage, int) else True,
                True,  # Disable interval
                current_stage,
            )

        # Progress interval - track real pipeline execution
        if trigger_id == "pipeline-progress-interval":
            logger.info(f"Interval fired - current_stage type: {type(current_stage)}, value: {current_stage}")

            # Check if this is a pipeline run ID (string hash) vs old simulation mode (int)
            if isinstance(current_stage, str) and len(current_stage) == 8:
                # REAL PIPELINE MODE
                pipeline_run_id = current_stage

                # Get pipeline data
                if not hasattr(register_pipeline_callbacks, 'pipeline_data'):
                    logger.error("Pipeline data not found!")
                    return no_update

                pipeline_data = register_pipeline_callbacks.pipeline_data.get(pipeline_run_id)
                if not pipeline_data:
                    logger.error(f"Pipeline run {pipeline_run_id} not found!")
                    return no_update

                current_stage_num = pipeline_data['current_stage']
                logger.info(f"Real pipeline mode - Run ID: {pipeline_run_id}, Stage: {current_stage_num}")

                # If pipeline already completed, stop the interval
                if current_stage_num >= 8:
                    return (
                        {"display": "block"},
                        100,
                        "ðŸ”´ REAL DATA MODE - Pipeline completed successfully!",
                        False,  # Enable run button
                        True,   # Disable stop button
                        False,  # Enable export button
                        False,  # Enable report button
                        True,   # Disable interval (STOP FIRING)
                        pipeline_run_id,
                    )

                # Process one stage at a time
                if current_stage_num < 8:
                    # Execute current stage
                    new_stage = current_stage_num + 1
                    success, result = _execute_pipeline_stage(
                        pipeline_data,
                        new_stage,
                        signal_data=pipeline_data['signal_data'],
                        fs=pipeline_data['fs'],
                        signal_type=pipeline_data['signal_type']
                    )

                    if success:
                        # Update pipeline data
                        pipeline_data['current_stage'] = new_stage
                        pipeline_data['results'][f'stage_{new_stage}'] = result

                        progress = (new_stage / 8) * 100
                        stage_names = [
                            "Data Ingestion",
                            "Quality Screening",
                            "Parallel Processing",
                            "Quality Validation",
                            "Segmentation",
                            "Feature Extraction",
                            "Intelligent Output",
                            "Output Package",
                        ]
                        status = f"ðŸ”´ REAL DATA MODE - Stage {new_stage}/8: {stage_names[new_stage - 1]}"

                        logger.info(f"âœ“ Stage {new_stage} completed: {stage_names[new_stage - 1]}")

                        # Check if pipeline is complete
                        if new_stage == 8:
                            logger.info("ðŸŽ‰ Pipeline completed successfully!")
                            return (
                                {"display": "block"},
                                100,
                                "ðŸ”´ REAL DATA MODE - Pipeline completed successfully!",
                                False,
                                True,
                                False,  # Enable export
                                False,  # Enable report
                                True,  # Disable interval
                                pipeline_run_id,
                            )

                        return (
                            {"display": "block"},
                            progress,
                            status,
                            True,
                            False,
                            True,
                            True,
                            False,  # Keep interval enabled
                            pipeline_run_id,
                        )
                    else:
                        # Stage failed
                        logger.error(f"âŒ Stage {new_stage} failed!")
                        return (
                            {"display": "block"},
                            (current_stage_num / 8) * 100,
                            f"Error at stage {new_stage}: {result}",
                            False,
                            True,
                            True,
                            True,
                            True,  # Disable interval
                            pipeline_run_id,
                        )

            # Fallback for old simulation mode (backward compatibility)
            # TODO: Re-enable after optimizing pipeline initialization
            # # Check if we have a session ID (real execution)
            # if isinstance(current_stage, str) and current_stage.startswith('session_'):
            #     session_id = current_stage.replace('session_', '')  # Extract actual session ID
            #     pipeline_service = get_pipeline_service()
            #     execution_state = pipeline_service.get_execution_state(session_id)
            #
            #     if execution_state:
            #         if execution_state.status == 'completed':
            #             return (
            #                 {"display": "block"},
            #                 100,
            #                 "Pipeline completed successfully!",
            #                 False,
            #                 True,
            #                 False,  # Enable export
            #                 False,  # Enable report
            #                 True,  # Disable interval
            #                 session_id,
            #             )
            #         elif execution_state.status == 'failed':
            #             return (
            #                 {"display": "block"},
            #                 execution_state.progress_percentage,
            #                 f"Pipeline failed: {execution_state.error_message}",
            #                 False,
            #                 True,
            #                 True,
            #                 True,
            #                 True,  # Disable interval
            #                 session_id,
            #             )
            #         elif execution_state.status == 'stopped':
            #             return (
            #                 {"display": "block"},
            #                 execution_state.progress_percentage,
            #                 f"Pipeline stopped at stage {execution_state.current_stage}",
            #                 False,
            #                 True,
            #                 True,
            #                 True,
            #                 True,  # Disable interval
            #                 session_id,
            #             )
            #         elif execution_state.status == 'running':
            #             return (
            #                 {"display": "block"},
            #                 execution_state.progress_percentage,
            #                 f"Stage {execution_state.current_stage}/8: {execution_state.stage_name}",
            #                 True,
            #                 False,
            #                 True,
            #                 True,
            #                 False,  # Keep interval enabled
            #                 session_id,
            #             )

            # Simulation mode - increment through stages
            if isinstance(current_stage, int) and current_stage < 8:
                logger.info(f"Simulation mode: incrementing from stage {current_stage}")
                new_stage = current_stage + 1
                progress = (new_stage / 8) * 100
                stage_names = [
                    "Data Ingestion",
                    "Quality Screening",
                    "Parallel Processing",
                    "Quality Validation",
                    "Segmentation",
                    "Feature Extraction",
                    "Intelligent Output",
                    "Output Package",
                ]
                status = f"ðŸŸ¡ SIMULATION MODE - Stage {new_stage}/8: {stage_names[new_stage - 1]}"

                # Check if pipeline is complete
                if new_stage == 8:
                    return (
                        {"display": "block"},
                        100,
                        "ðŸŸ¡ SIMULATION MODE - Pipeline completed successfully!",
                        False,
                        True,
                        False,  # Enable export
                        False,  # Enable report
                        True,  # Disable interval
                        new_stage,
                    )

                return (
                    {"display": "block"},
                    progress,
                    status,
                    True,
                    False,
                    True,
                    True,
                    False,  # Keep interval enabled
                    new_stage,
                )

        return no_update

    @app.callback(
        Output("pipeline-stage-details", "children"),
        [Input("pipeline-current-stage", "data")],
        [State("pipeline-signal-type", "value")],
    )
    def update_stage_details(current_stage, signal_type):
        """
        Update the stage details panel based on current pipeline stage.
        Shows real pipeline results when available, falls back to simulation data.
        """
        stage_num = _get_stage_number(current_stage)

        if stage_num == 0:
            return html.Div(
                [
                    html.P(
                        "Click 'Run Pipeline' to start processing.",
                        className="text-muted",
                    ),
                ],
                className="text-center p-5",
            )

        # Check if we have real pipeline data
        pipeline_data = None
        if isinstance(current_stage, str) and len(current_stage) == 8:
            # It's a pipeline_run_id, get real pipeline data
            if hasattr(register_pipeline_callbacks, 'pipeline_data'):
                pipeline_data = register_pipeline_callbacks.pipeline_data.get(current_stage)

        # Default simulation stage info (fallback)
        stage_info = {
            1: {
                "name": "Data Ingestion",
                "description": "Loading and validating input data, detecting format, and extracting metadata.",
                "metrics": {
                    "Format Detected": "CSV",
                    "Rows Loaded": "10,000",
                    "Sampling Rate": "1000 Hz",
                    "Duration": "10 seconds",
                },
            },
            2: {
                "name": "Quality Screening",
                "description": "Three-stage quality screening: SNR assessment, statistical analysis, and signal-specific checks.",
                "metrics": {
                    "SNR": "15.2 dB",
                    "Outlier Ratio": "0.05",
                    "Jump Ratio": "0.08",
                    "Overall Quality": "0.85",
                },
            },
            3: {
                "name": "Parallel Processing",
                "description": "Processing signal through multiple paths: RAW, FILTERED, and PREPROCESSED.",
                "metrics": {
                    "Paths Active": "2 (FILTERED, PREPROCESSED)",
                    "Filter Applied": "Butterworth bandpass 0.5-40 Hz",
                    "Artifacts Removed": "23",
                },
            },
            4: {
                "name": "Quality Validation",
                "description": "Comparing quality metrics across processing paths to select the best path.",
                "metrics": {
                    "RAW Quality": "0.65",
                    "FILTERED Quality": "0.82",
                    "PREPROCESSED Quality": "0.88",
                    "Best Path": "PREPROCESSED",
                },
            },
            5: {
                "name": "Segmentation",
                "description": "Dividing signal into overlapping windows for detailed analysis.",
                "metrics": {
                    "Window Size": "30 seconds",
                    "Overlap": "50%",
                    "Total Segments": "19",
                    "Valid Segments": "18",
                },
            },
            6: {
                "name": "Feature Extraction",
                "description": "Extracting time-domain, frequency-domain, and nonlinear features from each segment.",
                "metrics": {
                    "Time Features": "9",
                    "Frequency Features": "4",
                    "Nonlinear Features": "0",
                    "Total Features": "13",
                },
            },
            7: {
                "name": "Intelligent Output",
                "description": "Analyzing results and generating intelligent recommendations.",
                "metrics": {
                    "Best Path": "PREPROCESSED",
                    "Confidence": "High (0.92)",
                    "Recommendations": "3",
                },
            },
            8: {
                "name": "Output Package",
                "description": "Packaging all results, metadata, and recommendations for export.",
                "metrics": {
                    "Status": "Complete",
                    "Processing Time": "2.45 seconds",
                    "Output Size": "1.2 MB",
                },
            },
        }

        # If we have real pipeline data, use it instead of simulation data
        if pipeline_data and 'results' in pipeline_data:
            real_results = pipeline_data['results']
            
            # Update stage info with real data for completed stages
            if stage_num >= 1 and f'stage_{stage_num}' in real_results:
                stage_result = real_results[f'stage_{stage_num}']
                
                if stage_num == 1:  # Data Ingestion
                    stage_info[1]["metrics"] = {
                        "Samples": f"{stage_result.get('samples', 'N/A'):,}",
                        "Duration": f"{stage_result.get('duration', 0):.2f} seconds",
                        "Sampling Rate": f"{stage_result.get('fs', 0)} Hz",
                        "Signal Type": stage_result.get('signal_type', 'Unknown'),
                        "Mean": f"{stage_result.get('mean', 0):.3f}",
                        "Std Dev": f"{stage_result.get('std', 0):.3f}",
                    }
                elif stage_num == 2:  # Quality Screening
                    # Handle potential string values for numeric formatting
                    snr = stage_result.get('snr', 0)
                    outlier_ratio = stage_result.get('outlier_ratio', 0)
                    jump_ratio = stage_result.get('jump_ratio', 0)
                    overall_quality = stage_result.get('overall_quality', 0)
                    
                    try:
                        snr_str = f"{float(snr):.1f} dB"
                    except (ValueError, TypeError):
                        snr_str = f"{snr} dB"
                    
                    try:
                        outlier_ratio_str = f"{float(outlier_ratio):.3f}"
                    except (ValueError, TypeError):
                        outlier_ratio_str = f"{outlier_ratio}"
                    
                    try:
                        jump_ratio_str = f"{float(jump_ratio):.3f}"
                    except (ValueError, TypeError):
                        jump_ratio_str = f"{jump_ratio}"
                    
                    try:
                        overall_quality_str = f"{float(overall_quality):.3f}"
                    except (ValueError, TypeError):
                        overall_quality_str = f"{overall_quality}"
                    
                    stage_info[2]["metrics"] = {
                        "SNR": snr_str,
                        "Outlier Ratio": outlier_ratio_str,
                        "Jump Ratio": jump_ratio_str,
                        "Overall Quality": overall_quality_str,
                    }
                elif stage_num == 3:  # Parallel Processing
                    stage_info[3]["metrics"] = {
                        "Paths Active": f"{stage_result.get('paths_active', 0)}",
                        "Filter Applied": stage_result.get('filter_info', 'N/A'),
                        "Artifacts Removed": f"{stage_result.get('artifacts_removed', 0)}",
                    }
                elif stage_num == 4:  # Quality Validation
                    # Handle potential string values for numeric formatting
                    raw_quality = stage_result.get('raw_quality', 0)
                    filtered_quality = stage_result.get('filtered_quality', 0)
                    preprocessed_quality = stage_result.get('preprocessed_quality', 0)
                    
                    try:
                        raw_quality_str = f"{float(raw_quality):.3f}"
                    except (ValueError, TypeError):
                        raw_quality_str = f"{raw_quality}"
                    
                    try:
                        filtered_quality_str = f"{float(filtered_quality):.3f}"
                    except (ValueError, TypeError):
                        filtered_quality_str = f"{filtered_quality}"
                    
                    try:
                        preprocessed_quality_str = f"{float(preprocessed_quality):.3f}"
                    except (ValueError, TypeError):
                        preprocessed_quality_str = f"{preprocessed_quality}"
                    
                    stage_info[4]["metrics"] = {
                        "RAW Quality": raw_quality_str,
                        "FILTERED Quality": filtered_quality_str,
                        "PREPROCESSED Quality": preprocessed_quality_str,
                        "Best Path": stage_result.get('best_path', 'N/A'),
                    }
                elif stage_num == 5:  # Segmentation
                    # Handle potential string values for numeric formatting
                    window_size = stage_result.get('window_size', 0)
                    overlap_ratio = stage_result.get('overlap_ratio', 0)
                    total_segments = stage_result.get('total_segments', 0)
                    valid_segments = stage_result.get('valid_segments', 0)
                    
                    try:
                        overlap_percent = f"{float(overlap_ratio)*100:.0f}%"
                    except (ValueError, TypeError):
                        overlap_percent = f"{overlap_ratio}%"
                    
                    stage_info[5]["metrics"] = {
                        "Window Size": f"{window_size} seconds",
                        "Overlap": overlap_percent,
                        "Total Segments": f"{total_segments}",
                        "Valid Segments": f"{valid_segments}",
                    }
                elif stage_num == 6:  # Feature Extraction
                    stage_info[6]["metrics"] = {
                        "Time Features": f"{stage_result.get('time_features', 0)}",
                        "Frequency Features": f"{stage_result.get('frequency_features', 0)}",
                        "Nonlinear Features": f"{stage_result.get('nonlinear_features', 0)}",
                        "Total Features": f"{stage_result.get('total_features', 0)}",
                    }
                elif stage_num == 7:  # Intelligent Output
                    # Handle potential string values for numeric formatting
                    confidence = stage_result.get('confidence', 0)
                    
                    try:
                        confidence_str = f"{float(confidence):.3f}"
                    except (ValueError, TypeError):
                        confidence_str = f"{confidence}"
                    
                    stage_info[7]["metrics"] = {
                        "Best Path": stage_result.get('best_path', 'N/A'),
                        "Confidence": confidence_str,
                        "Recommendations": f"{stage_result.get('recommendations_count', 0)}",
                    }
                elif stage_num == 8:  # Output Package
                    # Handle potential string values for numeric formatting
                    processing_time = stage_result.get('processing_time', 0)
                    output_size = stage_result.get('output_size', 0)
                    
                    # Convert to float if possible, otherwise use string
                    try:
                        processing_time_str = f"{float(processing_time):.2f} seconds"
                    except (ValueError, TypeError):
                        processing_time_str = f"{processing_time} seconds"
                    
                    try:
                        output_size_str = f"{float(output_size):.1f} MB"
                    except (ValueError, TypeError):
                        output_size_str = f"{output_size} MB"
                    
                    stage_info[8]["metrics"] = {
                        "Status": "Complete",
                        "Processing Time": processing_time_str,
                        "Output Size": output_size_str,
                    }

        if stage_num in stage_info:
            info = stage_info[stage_num]
            
            # Add mode indicator
            mode_indicator = html.Div(
                [
                    dbc.Badge(
                        "ðŸ”´ REAL DATA MODE" if pipeline_data else "ðŸŸ¡ SIMULATION MODE",
                        color="success" if pipeline_data else "warning",
                        className="mb-2",
                        style={"fontSize": "0.9rem"}
                    )
                ],
                className="text-center"
            ) if stage_num > 0 else None
            
            return html.Div(
                [
                    mode_indicator,
                    html.H5(f"{stage_num}. {info['name']}", className="mb-3"),
                    html.P(info["description"], className="text-muted mb-3"),
                    html.H6("Stage Metrics:", className="mb-2"),
                    html.Div(
                        [
                            html.Div(
                                [
                                    html.Strong(f"{key}: "),
                                    html.Span(str(value)),
                                ],
                                className="mb-2",
                            )
                            for key, value in info["metrics"].items()
                        ]
                    ),
                ]
            )

        return html.Div("No stage information available.")

    @app.callback(
        Output("pipeline-paths-comparison", "figure"),
        [Input("pipeline-current-stage", "data")],
        [
            State("pipeline-paths", "value"),
            State("store-uploaded-data", "data"),
        ],
    )
    def update_paths_comparison(current_stage, selected_paths, uploaded_data):
        """
        Create comparison plot of different processing paths.
        Shows real pipeline data when available, falls back to simulation data.
        
        Technical Details:
        - Uses vitalDSP SignalFiltering.bandpass() for real bandpass filtering
        - Uses vitalDSP ArtifactRemoval.baseline_correction() for preprocessing
        - Signal-type-specific filter parameters (ECG: 0.5-40Hz, PPG: 0.5-8Hz, EEG: 0.5-50Hz)
        - Falls back to simple scaling if vitalDSP not available
        - Limits visualization to first 10 seconds for performance
        - Maintains proper time axis based on data's sampling frequency
        """
        # Only show after stage 3 (parallel processing)
        stage_num = _get_stage_number(current_stage)
        if stage_num < 3:
            return {
                "data": [],
                "layout": go.Layout(
                    title="Processing Paths Comparison",
                    xaxis={"title": "Time (s)"},
                    yaxis={"title": "Amplitude"},
                    annotations=[
                        {
                            "text": "Run pipeline to see path comparison",
                            "xref": "paper",
                            "yref": "paper",
                            "showarrow": False,
                            "font": {"size": 16, "color": "#999"},
                        }
                    ],
                ),
            }

        # Check if we have real pipeline data
        pipeline_data = None
        if isinstance(current_stage, str) and len(current_stage) == 8:
            # It's a pipeline_run_id, get real pipeline data
            if hasattr(register_pipeline_callbacks, 'pipeline_data'):
                pipeline_data = register_pipeline_callbacks.pipeline_data.get(current_stage)

        traces = []

        if pipeline_data and 'signal_data' in pipeline_data:
            # REAL DATA MODE: Use actual pipeline data
            signal_data = pipeline_data['signal_data']
            fs = pipeline_data['fs']
            
            # Create time axis
            t = np.arange(len(signal_data)) / fs
            
            # For real data, we'll show different processing stages
            # Since we don't have separate processed signals yet, we'll show the original signal
            # with different visual representations to simulate different processing paths
            
            if "raw" in selected_paths:
                traces.append(
                    go.Scatter(
                        x=t,
                        y=signal_data,
                        name="RAW",
                        mode="lines",
                        line=dict(color="red", width=1),
                        opacity=0.8,
                    )
                )

            if "filtered" in selected_paths:
                # Apply vitalDSP filtering to simulate filtered signal
                try:
                    from vitalDSP.filtering.signal_filtering import SignalFiltering
                    
                    # Create SignalFiltering object
                    sf = SignalFiltering(signal_data)
                    
                    # Apply bandpass filter based on signal type
                    if pipeline_data.get('signal_type', 'ecg').lower() == 'ecg':
                        lowcut, highcut = 0.5, 40.0
                    elif pipeline_data.get('signal_type', 'ecg').lower() == 'ppg':
                        lowcut, highcut = 0.5, 8.0
                    elif pipeline_data.get('signal_type', 'ecg').lower() == 'eeg':
                        lowcut, highcut = 0.5, 50.0
                    else:
                        lowcut, highcut = 0.5, 40.0
                    
                    # Use vitalDSP bandpass filtering
                    filtered_signal = sf.bandpass(lowcut=lowcut, highcut=highcut, fs=fs, order=4, filter_type="butter")
                    
                    traces.append(
                        go.Scatter(
                            x=t,
                            y=filtered_signal,
                            name="FILTERED",
                            mode="lines",
                            line=dict(color="blue", width=1),
                            opacity=0.8,
                        )
                    )
                except ImportError:
                    # Fallback if vitalDSP not available
                    traces.append(
                        go.Scatter(
                            x=t,
                            y=signal_data * 0.9,  # Simple scaling
                            name="FILTERED",
                            mode="lines",
                            line=dict(color="blue", width=1),
                            opacity=0.8,
                        )
                    )

            if "preprocessed" in selected_paths:
                # Apply vitalDSP preprocessing (filtering + artifact removal)
                try:
                    from vitalDSP.filtering.signal_filtering import SignalFiltering
                    from vitalDSP.filtering.artifact_removal import ArtifactRemoval
                    
                    # First apply vitalDSP filtering
                    sf = SignalFiltering(signal_data)
                    
                    # Apply bandpass filter based on signal type
                    if pipeline_data.get('signal_type', 'ecg').lower() == 'ecg':
                        lowcut, highcut = 0.5, 40.0
                    elif pipeline_data.get('signal_type', 'ecg').lower() == 'ppg':
                        lowcut, highcut = 0.5, 8.0
                    elif pipeline_data.get('signal_type', 'ecg').lower() == 'eeg':
                        lowcut, highcut = 0.5, 50.0
                    else:
                        lowcut, highcut = 0.5, 40.0
                    
                    filtered_signal = sf.bandpass(lowcut=lowcut, highcut=highcut, fs=fs, order=4, filter_type="butter")
                    
                    # Then apply vitalDSP artifact removal
                    ar = ArtifactRemoval(filtered_signal)
                    # Use baseline correction for preprocessing
                    preprocessed_signal = ar.baseline_correction(cutoff=0.5, fs=fs)
                    
                    traces.append(
                        go.Scatter(
                            x=t,
                            y=preprocessed_signal,
                            name="PREPROCESSED",
                            mode="lines",
                            line=dict(color="green", width=2),
                            opacity=0.9,
                        )
                    )
                except ImportError:
                    # Fallback if vitalDSP not available - simple baseline removal
                    preprocessed_signal = signal_data - np.mean(signal_data)
                    traces.append(
                        go.Scatter(
                            x=t,
                            y=preprocessed_signal,
                            name="PREPROCESSED",
                            mode="lines",
                            line=dict(color="green", width=2),
                            opacity=0.9,
                        )
                    )

            # Limit to first 10 seconds for visualization
            max_samples = min(len(signal_data), int(10 * fs))
            for trace in traces:
                trace.x = trace.x[:max_samples]
                trace.y = trace.y[:max_samples]

            title = f"ðŸ”´ REAL DATA MODE - Processing Paths Comparison (First 10 seconds)"
            
        else:
            # SIMULATION MODE: Generate sample data for demonstration
            t = np.linspace(0, 10, 1000)
            raw_signal = np.sin(2 * np.pi * 1.2 * t) + 0.3 * np.random.randn(len(t))
            filtered_signal = np.sin(2 * np.pi * 1.2 * t) + 0.1 * np.random.randn(len(t))
            preprocessed_signal = np.sin(2 * np.pi * 1.2 * t)

            if "raw" in selected_paths:
                traces.append(
                    go.Scatter(
                        x=t,
                        y=raw_signal,
                        name="RAW",
                        mode="lines",
                        line=dict(color="red", width=1),
                    )
                )

            if "filtered" in selected_paths:
                traces.append(
                    go.Scatter(
                        x=t,
                        y=filtered_signal,
                        name="FILTERED",
                        mode="lines",
                        line=dict(color="blue", width=1),
                    )
                )

            if "preprocessed" in selected_paths:
                traces.append(
                    go.Scatter(
                        x=t,
                        y=preprocessed_signal,
                        name="PREPROCESSED",
                        mode="lines",
                        line=dict(color="green", width=2),
                    )
                )

            title = f"ðŸŸ¡ SIMULATION MODE - Processing Paths Comparison (First 10 seconds)"

        return {
            "data": traces,
            "layout": go.Layout(
                title=title,
                xaxis={"title": "Time (s)"},
                yaxis={"title": "Amplitude"},
                hovermode="closest",
                legend={"x": 0, "y": 1},
            ),
        }

    @app.callback(
        Output("pipeline-quality-results", "children"),
        [Input("pipeline-current-stage", "data")],
    )
    def update_quality_results(current_stage):
        """
        Display quality screening results.
        """
        stage_num = _get_stage_number(current_stage)
        if stage_num < 2:
            return html.Div(
                "Quality screening not yet performed.", className="text-muted"
            )

        # Sample quality results
        quality_data = {
            "Stage 1: SNR Assessment": {"SNR (dB)": 15.2, "Status": "âœ… PASS"},
            "Stage 2: Statistical Screen": {
                "Outlier Ratio": 0.05,
                "Jump Ratio": 0.08,
                "Status": "âœ… PASS",
            },
            "Stage 3: Signal-Specific": {
                "Baseline Wander": 0.72,
                "Amplitude Variability": 0.81,
                "Status": "âœ… PASS",
            },
        }

        result_cards = []
        for stage_name, metrics in quality_data.items():
            result_cards.append(
                html.Div(
                    [
                        html.H6(stage_name, className="mb-2"),
                        html.Div(
                            [
                                html.Div(
                                    [html.Strong(f"{k}: "), html.Span(str(v))],
                                    className="mb-1",
                                )
                                for k, v in metrics.items()
                            ]
                        ),
                        html.Hr(),
                    ],
                    className="mb-3",
                )
            )

        return html.Div(result_cards)

    @app.callback(
        Output("pipeline-features-summary", "children"),
        [Input("pipeline-current-stage", "data")],
    )
    def update_features_summary(current_stage):
        """
        Display feature extraction summary.
        """
        stage_num = _get_stage_number(current_stage)
        if stage_num < 6:
            return html.Div(
                "Feature extraction not yet performed.", className="text-muted"
            )

        # Sample feature summary
        features_df = pd.DataFrame(
            {
                "Feature": [
                    "Mean",
                    "Std Dev",
                    "RMS",
                    "Spectral Centroid",
                    "Dominant Frequency",
                ],
                "Value": [0.023, 0.145, 0.147, 1.85, 1.2],
                "Unit": ["V", "V", "V", "Hz", "Hz"],
            }
        )

        return html.Div(
            [
                html.H6("Extracted Features (Sample)", className="mb-3"),
                html.Div(
                    [
                        html.Div(
                            [
                                html.Strong(f"{row['Feature']}: "),
                                html.Span(f"{row['Value']:.3f} {row['Unit']}"),
                            ],
                            className="mb-2",
                        )
                        for _, row in features_df.iterrows()
                    ]
                ),
            ]
        )

    @app.callback(
        Output("pipeline-output-recommendations", "children"),
        [Input("pipeline-current-stage", "data")],
    )
    def update_output_recommendations(current_stage):
        """
        Display intelligent output recommendations.
        """
        stage_num = _get_stage_number(current_stage)
        if stage_num < 7:
            return html.Div(
                "Recommendations not yet generated.", className="text-muted"
            )

        return html.Div(
            [
                html.H6("Processing Recommendations:", className="mb-3"),
                html.Ul(
                    [
                        html.Li(
                            [
                                html.Strong("Best Path: "),
                                "PREPROCESSED (Quality: 0.88, Confidence: High)",
                            ]
                        ),
                        html.Li(
                            [
                                html.Strong("Signal Quality: "),
                                "Good - suitable for detailed analysis",
                            ]
                        ),
                        html.Li(
                            [
                                html.Strong("Recommendation: "),
                                "Use PREPROCESSED path for HRV analysis and feature extraction",
                            ]
                        ),
                    ]
                ),
            ]
        )

    @app.callback(
        Output("pipeline-results", "data"),
        [Input("pipeline-current-stage", "data")],
    )
    def update_pipeline_results(current_stage):
        """
        Update pipeline results when execution completes.
        """
        # Real pipeline mode - return actual results
        if isinstance(current_stage, str) and len(current_stage) == 8:
            pipeline_run_id = current_stage
            if hasattr(register_pipeline_callbacks, 'pipeline_data'):
                pipeline_data = register_pipeline_callbacks.pipeline_data.get(pipeline_run_id)
                if pipeline_data and pipeline_data.get('current_stage') == 8:
                    return {
                        'pipeline_run_id': pipeline_run_id,
                        'status': 'completed',
                        'results': pipeline_data.get('results', {}),
                        'signal_type': pipeline_data.get('signal_type'),
                        'paths': pipeline_data.get('paths'),
                    }

        # Legacy simulation mode - return mock results when complete
        if isinstance(current_stage, int) and current_stage == 8:
            return {
                'status': 'completed',
                'simulation': True,
                'execution_time': 2.45
            }

        return no_update

    @app.callback(
        [
            Output("pipeline-progress-container", "children", allow_duplicate=True),
            Output("pipeline-progress-container", "style", allow_duplicate=True),
        ],
        [Input("pipeline-current-stage", "data")],
        prevent_initial_call=True,
    )
    def update_pipeline_step_indicator(current_stage):
        """
        Update the visual step progress indicator based on current stage.
        """
        from dash import html

        # Handle both int stages (1-8) and string pipeline_run_id
        stage_num = _get_stage_number(current_stage)

        # Define the 8 pipeline stages
        pipeline_stages = [
            "Data Ingestion",
            "Quality Screening",
            "Parallel Processing",
            "Quality Validation",
            "Segmentation",
            "Feature Extraction",
            "Intelligent Output",
            "Output Package",
        ]

        # Create step items manually (instead of using helper function)
        step_items = []
        for i, step_name in enumerate(pipeline_stages):
            # Determine step status based on current stage
            if i < stage_num:
                # Completed
                icon = "fa-check-circle"
                icon_color = "text-success"
                status = "completed"
            elif i == stage_num:
                # In progress
                icon = "fa-circle-notch fa-spin"
                icon_color = "text-primary"
                status = "in-progress"
            else:
                # Pending
                icon = "fa-circle"
                icon_color = "text-muted"
                status = "pending"

            step_items.append(
                html.Div(
                    className=f"step-item {status}",
                    style={
                        "display": "inline-block",
                        "textAlign": "center",
                        "flex": "1",
                        "position": "relative",
                    },
                    children=[
                        html.I(
                            className=f"fas {icon} {icon_color}",
                            style={"fontSize": "24px"},
                        ),
                        html.Div(
                            step_name,
                            className="mt-2",
                            style={
                                "fontSize": "12px",
                                "fontWeight": "bold" if i == stage_num else "normal",
                            },
                        ),
                    ],
                )
            )

            # Add connector line between steps (except after last step)
            if i < len(pipeline_stages) - 1:
                connector_color = "#28a745" if i < stage_num else "#dee2e6"
                step_items.append(
                    html.Div(
                        style={
                            "flex": "0.5",
                            "height": "2px",
                            "backgroundColor": connector_color,
                            "alignSelf": "center",
                            "marginTop": "-20px",
                        }
                    )
                )

        # Create the progress indicator content
        progress_content = html.Div(
            className="step-progress",
            style={
                "display": "flex",
                "alignItems": "flex-start",
                "justifyContent": "space-between",
                "padding": "20px",
                "backgroundColor": "#f8f9fa",
                "borderRadius": "5px",
                "marginBottom": "20px",
            },
            children=step_items,
        )

        # Show the container when pipeline is running (stage > 0)
        container_style = {"display": "block" if stage_num > 0 else "none"}

        return progress_content, container_style

    logger.info("Pipeline callbacks registered successfully")
